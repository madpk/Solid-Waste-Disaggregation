# Load the AutoML model used in the study
aml <- h2o.loadModel("V3_DeepLearning_grid_3")
# Get variable importance (only for models that support it, like XGBoost, GBM)
var_imp <- h2o.varimp(aml )
# Create the variable importance data frame
var_imp <- data.frame(
variable = c("Building Count", "Population Density", "RESBU Volume",
"NRESBU Volume"),
percentage = c(0.335697, 0.261192, 0.275932, 0.127179) * 100  # Convert to %
)
# Plot the variable importance
ggplot(var_imp, aes(x = reorder(variable, percentage), y = percentage)) +
geom_bar(stat = "identity", fill = "#69b3a2", color = "black", width = 0.6) +
coord_flip() +
labs(
x = "Variable",
y = "Importance (%)",
title = "Relative Variable Importance"
) +
geom_text(
aes(label = sprintf("%.1f%%", percentage)),
hjust = -0.2,
size = 5,
fontface = "bold"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14, face = "bold"),
panel.grid.major.y = element_blank(),  # Optional: cleaner look
panel.grid.minor = element_blank()
) +
ylim(0, max(var_imp$percentage) + 10)  # Add space for labels above bars
# Convert the polygon to 'terra' format and split into individual admins
multipolygon <- vect(kerala_admin)
multipolygon_list <- split(multipolygon, seq(nrow(multipolygon)))
# Convert raster stack into a list
stacked_rasters <- as.list(raster_items)
# Function to crop and mask raster with a polygon
crop_rast <- function(input_raster, multipolygon) {
cropped <- terra::crop(input_raster, multipolygon)
masked <- terra::mask(cropped, multipolygon)
}
# Function to normalize raster using min-max scaling
normalize_raster <- function(raster_layer) {
min_val <- global(raster_layer, "min", na.rm = TRUE)[1, 1]
max_val <- global(raster_layer, "max", na.rm = TRUE)[1, 1]
(raster_layer - min_val) / (max_val - min_val)
}
# Crop, mask, and normalize all raster layers for each polygon
cropped_normalized_rasters <- map(multipolygon_list, function(polygon) {
map(stacked_rasters, ~ {
cropped <- crop_rast(.x, polygon)
normalize_raster(cropped)
})
})
# Define variable weights based on variable importance (must match order)
weights <- c(0.335697, 0.127179, 0.275932, 0.261192)
# Apply weights to each normalized raster layer
weighted_rasters <- map(cropped_normalized_rasters, function(raster_list) {
map2(raster_list, weights, function(raster, weight) {
raster * weight
})
})
# Sum weighted rasters to create composite raster for each admin
sum_rasters <- map(weighted_rasters, function(raster_list) {
rast(raster_list) %>% app(sum, na.rm = TRUE)
})
# Scale raster so that each admin raster sums to 1 (i.e., total weight = 1)
scale_to_sum_one <- function(raster) {
total_sum <- sum(values(raster), na.rm = TRUE)
raster / total_sum
}
scaled_rasters <- map(sum_rasters, scale_to_sum_one)
# Disaggregate solid waste (SW_Ton) into the raster cells using scaled weights
disaggregated_waste_rasters <- map2(kerala_admin$SW_Ton, scaled_rasters,
function(total_waste, raster) {
raster * total_waste
})
# Combine all admin rasters into a single raster layer
raster_collection <- sprc(disaggregated_waste_rasters)
merged_raster <- merge(raster_collection)
plot(merged_raster)
sum_of_pixels <- global(merged_raster, sum, na.rm = TRUE)
sum_of_pixels
# ------------------------------------------------------------------------------
# Automated Machine Learning-Based Spatial Disaggregation Framework
# Author: Madhuraj Palat Kannakai
# GitHub: https://github.com/madpk
# ------------------------------------------------------------------------------
# Set Working Directory
wd<-setwd("C:/Users/madhu/OneDrive/Documents/GitHub/-Solid-Waste-Disaggregation/Datasets")
# Load Required Libraries
library(tidyverse)    # Data manipulation and ggplot2
library(sf)           # Spatial data handling
library(terra)        # Raster and spatial data processing
library(exactextractr)# Efficient raster extraction
library(h2o)          # Automated Machine Learning
library(FactoMineR)   # Principal Component Analysis
library(factoextra)   # PCA Visualization
library(corrplot)     # Correlation matrix visualization
# Load Kerala admin Multipolygon
kerala_admin <- st_read("Kerala_admin_multipolygon.geojson")
# Reproject to UTM Zone 43N (EPSG:32643)
kerala_admin <- st_transform(kerala_admin, crs = 32643)
# List all .tif raster files in the working directory
raster_files <- list.files(wd, pattern = "\\.tif$", full.names = TRUE)
# Load all rasters as SpatRaster objects using terra::rast()
raster_items <- lapply(raster_files, rast)
# Define the extraction function (sum of raster values within each admin)
extract_raster_sum <- function(raster, admin_vect) {
exact_extract(raster, admin_vect, 'sum')
}
# Apply the extraction function to each raster in the list
admin_data_sum <- map(raster_items, ~extract_raster_sum(.x, kerala_admin))
# Assign descriptive names to the extracted data
names(admin_data_sum) <- c("builtcount_sum",
"builtvolnres_sum",
"builtvolres_sum",
"popdens_sum")
# Combine extracted values with original spatial data
admin_data_sum_comb <- bind_cols(as.data.frame(admin_data_sum), kerala_admin)
# Create a Machine Learning-ready dataframe (remove geometry, retain only values)
admin_data_summl <- admin_data_sum_comb %>%
st_drop_geometry() %>%
select(builtcount_sum, builtvolnres_sum, builtvolres_sum, popdens_sum, SW_Ton)
# Preview the final ML-ready dataframe
glimpse(admin_data_summl)
# Combine ML features with admin labels
pca_data <- data.frame(admin_data_summl, admin = factor(admin_data_sum_comb$LSGI))
# Rename columns for clarity in visualizations
colnames(pca_data) <- c("Building count",
"NRESBU volume",
"RESBU volume",
"Population density",
"Solid waste generation",
"admin")
pca_result <- PCA(pca_data[1:5], graph = FALSE)  # Only numeric variables
# PCA Biplot
fviz_pca_biplot(pca_result,
col.ind = pca_data$admin,
palette = "lancet",
shape.ind = 16,
pointshape = 16,
pointsize = 4,
alpha.ind = 0.8,
addEllipses = FALSE,
repel = TRUE,
label = "var",
mean.point = FALSE,
legend.title = "admin")
# Compute Spearman correlation matrix
spearman_corr <- cor(pca_data[, 1:5], method = "spearman")
# Display correlation matrix with numbers
corrplot(spearman_corr,
method = "number",
number.cex = 0.8)
# Initialize H2O cluster (skip if already initialized)
h2o.init()
# Load the AutoML model used in the study
aml <- h2o.loadModel("V3_DeepLearning_grid_3")
# View the model's parameters
aml_model_params <- aml@parameters
# Get variable importance (only for models that support it, like XGBoost, GBM)
var_imp <- h2o.varimp(aml )
# Create the variable importance data frame
var_imp <- data.frame(
variable = c("Building Count", "Population Density", "RESBU Volume",
"NRESBU Volume"),
percentage = c(0.335697, 0.261192, 0.275932, 0.127179) * 100  # Convert to %
)
# Plot the variable importance
ggplot(var_imp, aes(x = reorder(variable, percentage), y = percentage)) +
geom_bar(stat = "identity", fill = "#69b3a2", color = "black", width = 0.6) +
coord_flip() +
labs(
x = "Variable",
y = "Importance (%)",
title = "Relative Variable Importance"
) +
geom_text(
aes(label = sprintf("%.1f%%", percentage)),
hjust = -0.2,
size = 5,
fontface = "bold"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14, face = "bold"),
panel.grid.major.y = element_blank(),  # Optional: cleaner look
panel.grid.minor = element_blank()
) +
ylim(0, max(var_imp$percentage) + 10)  # Add space for labels above bars
# Convert the polygon to 'terra' format and split into individual admins
multipolygon <- vect(kerala_admin)
multipolygon_list <- split(multipolygon, seq(nrow(multipolygon)))
# Convert raster stack into a list
stacked_rasters <- as.list(raster_items)
# Function to crop and mask raster with a polygon
crop_rast <- function(input_raster, multipolygon) {
cropped <- terra::crop(input_raster, multipolygon)
masked <- terra::mask(cropped, multipolygon)
}
# Function to normalize raster using min-max scaling
normalize_raster <- function(raster_layer) {
min_val <- global(raster_layer, "min", na.rm = TRUE)[1, 1]
max_val <- global(raster_layer, "max", na.rm = TRUE)[1, 1]
(raster_layer - min_val) / (max_val - min_val)
}
# Crop, mask, and normalize all raster layers for each polygon
cropped_normalized_rasters <- map(multipolygon_list, function(polygon) {
map(stacked_rasters, ~ {
cropped <- crop_rast(.x, polygon)
normalize_raster(cropped)
})
})
plot(merged_raster)
# Function to normalize raster using min-max scaling
normalize_raster <- function(raster_layer) {
min_val <- global(raster_layer, "min", na.rm = TRUE)[1, 1]
max_val <- global(raster_layer, "max", na.rm = TRUE)[1, 1]
(raster_layer - min_val) / (max_val - min_val)
}
# Crop, mask, and normalize all raster layers for each polygon
cropped_normalized_rasters <- map(multipolygon_list, function(polygon) {
map(stacked_rasters, ~ {
cropped <- crop_rast(.x, polygon)
normalize_raster(cropped)
})
})
# Define variable weights based on variable importance (must match order)
weights <- c(0.335697, 0.127179, 0.275932, 0.261192)
# Apply weights to each normalized raster layer
weighted_rasters <- map(cropped_normalized_rasters, function(raster_list) {
map2(raster_list, weights, function(raster, weight) {
raster * weight
})
})
# Sum weighted rasters to create composite raster for each admin
sum_rasters <- map(weighted_rasters, function(raster_list) {
rast(raster_list) %>% app(sum, na.rm = TRUE)
})
# Scale raster so that each admin raster sums to 1 (i.e., total weight = 1)
scale_to_sum_one <- function(raster) {
total_sum <- sum(values(raster), na.rm = TRUE)
raster / total_sum
}
scaled_rasters <- map(sum_rasters, scale_to_sum_one)
# Disaggregate solid waste (SW_Ton) into the raster cells using scaled weights
disaggregated_waste_rasters <- map2(kerala_admin$SW_Ton, scaled_rasters,
function(total_waste, raster) {
raster * total_waste
})
# Combine all admin rasters into a single raster layer
raster_collection <- sprc(disaggregated_waste_rasters)
merged_raster <- merge(raster_collection)
plot(merged_raster)
aml_model_params
aml
aml
#Split into test and train datasets
splits <- h2o.splitFrame(admin_data_summl_h2o, ratios = 0.7, seed = 1234)
#Split into test and train datasets
splits <- h2o.splitFrame(admin_data_summl, ratios = 0.7, seed = 1234)
#Split into test and train datasets
admin_data_summl_h2o <- as.h2o(lsgi_data_summl)
#Split into test and train datasets
admin_data_summl_h2o <- as.h2o(admin_data_summl)
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
train <- splits[[1]]
test <- splits[[2]]
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
#Split into test and train datasets
admin_data_summl_h2o <- as.h2o(admin_data_summl)
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
train <- splits[[1]]
test <- splits[[2]]
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
#Split into test and train datasets
admin_data_summl_h2o <- as.h2o(admin_data_summl)
admin_data_summl_h2o
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
#Split into test and train datasets
admin_data_summl_h20 <- as.h2o(admin_data_summl)
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
train <- splits[[1]]
train <- splits[[1]]
test <- splits[[2]]
train
#Performance on test data
test_performance <- h2o.performance(aml, newdata = test)
test_performance
r_squared <- h2o.r2(aml)
r_squared
test_performance
# Initialize H2O
h2o.init()
# Convert data frame to H2O object
admin_data_summl_h20 <- as.h2o(admin_data_summl)
# Split data: 70% training, 30% testing
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
train <- splits[[1]]
test <- splits[[2]]
# Load Pre-Trained AutoML Model (DeepLearning_grid_3_AutoML_13_20250315_154642_model_2)
aml <- h2o.loadModel("V3_DeepLearning_grid_3")
# View the model's parameters
aml_model_params <- aml@parameters
#Performance on test data
test_performance <- h2o.performance(aml, newdata = test)
# Get variable importance (only for models that support it, like XGBoost, GBM)
var_imp <- h2o.varimp(aml )
# Create the variable importance data frame
var_imp <- data.frame(
variable = c("Building Count", "Population Density", "RESBU Volume",
"NRESBU Volume"),
percentage = c(0.335697, 0.261192, 0.275932, 0.127179) * 100  # Convert to %
)
aml
aml_model_params
test_performance
# ------------------------------------------------------------------------------
# Automated Machine Learning-Based Spatial Disaggregation Framework
# Author: Madhuraj Palat Kannakai
# GitHub: https://github.com/madpk
# ------------------------------------------------------------------------------
# Set Working Directory
wd<-setwd("C:/Users/madhu/OneDrive/Documents/GitHub/-Solid-Waste-Disaggregation/Datasets")
# Load Required Libraries
library(tidyverse)    # Data manipulation and ggplot2
library(sf)           # Spatial data handling
library(terra)        # Raster and spatial data processing
library(exactextractr)# Efficient raster extraction
library(h2o)          # Automated Machine Learning
library(FactoMineR)   # Principal Component Analysis
library(factoextra)   # PCA Visualization
library(corrplot)     # Correlation matrix visualization
# Load Kerala admin Multipolygon
kerala_admin <- st_read("Kerala_admin_multipolygon.geojson")
# Reproject to UTM Zone 43N (EPSG:32643)
kerala_admin <- st_transform(kerala_admin, crs = 32643)
# List all .tif raster files in the working directory
raster_files <- list.files(wd, pattern = "\\.tif$", full.names = TRUE)
# Load all rasters as SpatRaster objects using terra::rast()
raster_items <- lapply(raster_files, rast)
# Define the extraction function (sum of raster values within each admin)
extract_raster_sum <- function(raster, admin_vect) {
exact_extract(raster, admin_vect, 'sum')
}
# Apply the extraction function to each raster in the list
admin_data_sum <- map(raster_items, ~extract_raster_sum(.x, kerala_admin))
# Assign descriptive names to the extracted data
names(admin_data_sum) <- c("builtcount_sum",
"builtvolnres_sum",
"builtvolres_sum",
"popdens_sum")
# Combine extracted values with original spatial data
admin_data_sum_comb <- bind_cols(as.data.frame(admin_data_sum), kerala_admin)
# Create a Machine Learning-ready dataframe (remove geometry, retain only values)
admin_data_summl <- admin_data_sum_comb %>%
st_drop_geometry() %>%
select(builtcount_sum, builtvolnres_sum, builtvolres_sum, popdens_sum, SW_Ton)
# Preview the final ML-ready dataframe
glimpse(admin_data_summl)
# Combine ML features with admin labels
pca_data <- data.frame(admin_data_summl, admin = factor(admin_data_sum_comb$LSGI))
# Rename columns for clarity in visualizations
colnames(pca_data) <- c("Building count",
"NRESBU volume",
"RESBU volume",
"Population density",
"Solid waste generation",
"admin")
pca_result <- PCA(pca_data[1:5], graph = FALSE)  # Only numeric variables
# PCA Biplot
fviz_pca_biplot(pca_result,
col.ind = pca_data$admin,
palette = "lancet",
shape.ind = 16,
pointshape = 16,
pointsize = 4,
alpha.ind = 0.8,
addEllipses = FALSE,
repel = TRUE,
label = "var",
mean.point = FALSE,
legend.title = "admin")
# Compute Spearman correlation matrix
spearman_corr <- cor(pca_data[, 1:5], method = "spearman")
# Display correlation matrix with numbers
corrplot(spearman_corr,
method = "number",
number.cex = 0.8)
# Initialize H2O
h2o.init()
# Convert data frame to H2O object
admin_data_summl_h20 <- as.h2o(admin_data_summl)
# Split data: 70% training, 30% testing
splits <- h2o.splitFrame(admin_data_summl_h20, ratios = 0.7, seed = 1234)
train <- splits[[1]]
test <- splits[[2]]
# Load Pre-Trained AutoML Model (DeepLearning_grid_3_AutoML_13_20250315_154642_model_2)
aml <- h2o.loadModel("V3_DeepLearning_grid_3")
# View the model's parameters
aml_model_params <- aml@parameters
#Performance on test data
test_performance <- h2o.performance(aml, newdata = test)
# Get variable importance (only for models that support it, like XGBoost, GBM)
var_imp <- h2o.varimp(aml )
# Create the variable importance data frame
var_imp <- data.frame(
variable = c("Building Count", "Population Density", "RESBU Volume",
"NRESBU Volume"),
percentage = c(0.335697, 0.261192, 0.275932, 0.127179) * 100  # Convert to %
)
# Plot the variable importance
ggplot(var_imp, aes(x = reorder(variable, percentage), y = percentage)) +
geom_bar(stat = "identity", fill = "#69b3a2", color = "black", width = 0.6) +
coord_flip() +
labs(
x = "Variable",
y = "Importance (%)",
title = "Relative Variable Importance"
) +
geom_text(
aes(label = sprintf("%.1f%%", percentage)),
hjust = -0.2,
size = 5,
fontface = "bold"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14, face = "bold"),
panel.grid.major.y = element_blank(),  # Optional: cleaner look
panel.grid.minor = element_blank()
) +
ylim(0, max(var_imp$percentage) + 10)  # Add space for labels above bars
# Convert the polygon to 'terra' format and split into individual admins
multipolygon <- vect(kerala_admin)
multipolygon_list <- split(multipolygon, seq(nrow(multipolygon)))
# Convert raster stack into a list
stacked_rasters <- as.list(raster_items)
# Function to crop and mask raster with a polygon
crop_rast <- function(input_raster, multipolygon) {
cropped <- terra::crop(input_raster, multipolygon)
masked <- terra::mask(cropped, multipolygon)
}
# Function to normalize raster using min-max scaling
normalize_raster <- function(raster_layer) {
min_val <- global(raster_layer, "min", na.rm = TRUE)[1, 1]
max_val <- global(raster_layer, "max", na.rm = TRUE)[1, 1]
(raster_layer - min_val) / (max_val - min_val)
}
# Crop, mask, and normalize all raster layers for each polygon
cropped_normalized_rasters <- map(multipolygon_list, function(polygon) {
map(stacked_rasters, ~ {
cropped <- crop_rast(.x, polygon)
normalize_raster(cropped)
})
})
# Define variable weights based on variable importance (must match order)
weights <- c(0.335697, 0.127179, 0.275932, 0.261192)
# Apply weights to each normalized raster layer
weighted_rasters <- map(cropped_normalized_rasters, function(raster_list) {
map2(raster_list, weights, function(raster, weight) {
raster * weight
})
})
# Sum weighted rasters to create composite raster for each admin
sum_rasters <- map(weighted_rasters, function(raster_list) {
rast(raster_list) %>% app(sum, na.rm = TRUE)
})
# Scale raster so that each admin raster sums to 1 (i.e., total weight = 1)
scale_to_sum_one <- function(raster) {
total_sum <- sum(values(raster), na.rm = TRUE)
raster / total_sum
}
aml
aml_model_params
test_performance
r_squared <- h2o.r2(aml)
r_squared
r_squared <- h2o.r2(aml)
r_squared
r2_test <- h2o.r2(test_performance)
r2_test
aml_model_params
# Load Pre-Trained AutoML Model (DeepLearning_grid_3_AutoML_13_20250315_154642_model_2)
aml <- h2o.loadModel("V3_DeepLearning_grid_3")
# View the model's parameters
aml_model_params <- aml@parameters
#Performance on test data
test_performance <- h2o.performance(aml, newdata = test)
r_squared <- h2o.r2(aml)
r2_test <- h2o.r2(test_performance)
aml
aml_model_params
test_performance
r_squared
r2_test
r2_test
aml
aml_model_params
test_performance
r_squared
r2_test
var_imp
r_train <- h2o.r2(aml)
